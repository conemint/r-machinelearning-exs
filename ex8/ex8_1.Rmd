---
title: "Programming Exercise 8: Anomaly Detection and Recommender Systems"
author: "Min Z"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_notebook: default
  html_document: default
---


# Introduction
In this exercise, you will implement the anomaly detection algorithm and apply it to detect failing servers on a network. 

**In this part, we have a crossvalidation set with labeled data that answers the question:
How small the P value can be seen as wrong?**

* In the second part, you will use collaborative filtering to build a recommender system for movies. 

## 1 Anomaly detection

In this exercise, you will implement an anomaly detection algorithm to detect anomalous behavior in server computers. The features measure the through- put (mb/s) and latency (ms) of response of each server. While your servers were operating, you collected m = 307 examples of how they were behaving, and thus have an unlabeled dataset {x(1),...,x(m)}. You suspect that the vast majority of these examples are “normal” (non-anomalous) examples of the servers operating normally, but there might also be some examples of servers acting anomalously within this dataset.

You will use a Gaussian model to detect anomalous examples in your dataset. You will first start on a 2D dataset that will allow you to visualize what the algorithm is doing. On that dataset you will fit a Gaussian dis- tribution and then find values that have very low probability and hence can be considered anomalies. After that, you will apply the anomaly detection algorithm to a larger dataset with many dimensions. 

Now, first read in data and visualize:

```{r}
# remove all list
rm(list = ls())
```

```{r}
library(R.matlab)
ex8data1 <- readMat("ex8data1.mat")
X<-ex8data1$X
plot(X)
```
### 1.1 Gaussian distribution

To perform anomaly detection, you will first need to fit a model to the data’s distribution. Throughput (mb/s)
Given a training set {x(1),...,x(m)} (where x(i) ∈ Rn), you want to estimate the Gaussian distribution for each of the features xi. For each feature i = 1...n, you need to find parameters μi and σi2 that fit the data in the
i-th dimension {x(1), ..., x(m)} (the i-th dimension of each example). 
The Gaussian distribution is given by:
$$p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
where μ is the mean and σ2 controls the variance.


### 1.2 Estimating parameters for a Gaussian

For this exercise, we assume a Gaussian distribution for the dataset.

We first estimate the parameters of our assumed Gaussian distribution, then compute the probabilities for each of the points and then visualize both the overall distribution and where each of the points falls in terms of that distribution.

You can estimate the parameters, (μi, σi2), of the i-th feature by using the following equations. To estimate the mean, you will use:
$$\mu_i=\frac{1}{m}\sum_{j=1}^mx_i^{(j)}$$
and for the variance you will use:

$$\sigma_i^2=\frac{1}{m}\sum_{j=1}^m(x_i^{(j)}-\mu_i)^2$$
```{r}
# Estimate mu and sigma2
mu = apply(X,2,mean)
sigma2 = apply(X,2,sd)^2 *((dim(X)[1]-1)/dim(X)[1])
```

Look at the density distribution of 2 variables
```{r}
x <- seq(-4,4,length=100)*sigma2^.5 + mu
hx <- dnorm(x,mu,sigma2^.5)
plot(x, hx)
```


Calculate the density of the multivariate normal at each data point (row) of X
$$p(x)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j^2)^2}{2\sigma_j^2})$$
Or, use dnorm() from stats package to calculate p(xj), and do prod for each feature:
```{r}
# dnorm gives the normal distribution density, on one obs of x
dnorm.mat<-t(apply(X,1,dnorm,mu,sigma2^0.5))
p = apply(dnorm.mat,1,prod)
# alternatively, calculate p(x) from mu and sigma
# multivariateGaussian<-function(X, mu, Sigma2){
#   k = length(mu);
# 
#   if (length(Sigma2) == 1){
#     Sigma2 = diag(k)*Sigma2
#   }
#   X_mu=sweep(X,2,mu,"-")^2
#   X_mu_dsig= exp(-sweep(X_mu,2,Sigma2,"/")/2)
#   pxj = sweep(X_mu_dsig,2, ((2*pi*Sigma2)^0.5),"/")
#   p = apply(pxj,1,prod)
#   return(p)
# }
# p = multivariateGaussian(X, mu, sigma2)

```

Visualize:
```{r}
covx<-cov(X)
bivn <- mvrnorm(5000, mu = mu, Sigma = covx )  # from Mass package

bivn.kde <- kde2d(bivn[,1], bivn[,2], n = 50)  # from MASS package
plot(X,col = "blue")
contour(bivn.kde,levels = c(.5,.10,.05,.01,.001,.0001), add = TRUE)     # from base graphics package
```
another:
```{r}
persp(bivn.kde, phi = 30, theta = 20, d = 5) # from graphics package
```

### 1.3 Selecting the threshold, ε

Now that you have estimated the Gaussian parameters, you can investigate which examples have a very high probability given this distribution and which examples have a very low probability. The low probability examples are more likely to be the anomalies in our dataset. One way to determine which examples are anomalies is to select a threshold based on a cross validation set.

For this, we will use a cross validation set {(x(1), y(1)), . . . , (x(mcv), y(mcv))}, where the label y = 1 corresponds to an anomalous example, and y = 0 corresponds to a normal example.

For each cross validation example, we will compute p(x(i)). The vector of all of these probabilities p(x(1)), . . . , p(x(mcv) ) is passed to selectThreshold.m in the vector pval. The corresponding labels y(1), . . . , y(mcv) is passed to the same function in the vector yval.

Now you will find a good epsilon threshold using a cross-validation set probabilities given the estimated Gaussian distribution.

Calculate the $\mu$ and $\sigma^2$ for cross validation set:
```{r}
Xval<-ex8data1$Xval

dnormval.mat<-t(apply(Xval,1,dnorm,mu,sigma2^0.5))
pval = apply(dnormval.mat,1,prod)
# pval = multivariateGaussian(Xval, mu, sigma2)
```

Then select threshold:
```{r}
selectThreshold<-function(yval, pval){
  # %SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting
  # %outliers
  # %   [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best
  # %   threshold to use for selecting outliers based on the results from a
  # %   validation set (pval) and the ground truth (yval).
  mabestEpsilon = 0
  bestF1 = 0
  F1 = 0
  stepsize = (max(pval) - min(pval)) / 1000;
  for(epsilon in seq(from=min(pval), to=max(pval), by=stepsize)){
    cvPredictions = (pval < epsilon)
    
    tp = sum((cvPredictions == T) & (yval == 1))
    fp = sum((cvPredictions == T) & (yval == 0))
    fn = sum((cvPredictions == F) & (yval == 1))
    
    if (tp ==0){
      F1 =0
    }else{
      prec = tp/(tp+fp)
      rec = tp/(tp+fn)
      F1 = 2*prec*rec/ (prec+rec)
    }

    if(F1 > bestF1){
      bestF1 = F1
      bestEpsilon = epsilon
    }  
  }
  returnls = list("epsilon"=bestEpsilon,"F1"=bestF1)
  return(returnls)
}
```

```{r}
rtls = selectThreshold(ex8data1$yval, pval)
```

Best epsilon found using cross-validation: `r rtls$epsilon`

Best F1 on Cross Validation Set: `r rtls$F1`

(you should see a value epsilon of about 8.99e-05)

Find the outliers in the training set 
```{r}
outliers = X[which(p < rtls$epsilon),]
```

And plot the outliers:
```{r}
library(ggplot2)
data.df<-as.data.frame(X)
outer.df<-as.data.frame(outliers)
pic<-ggplot() + geom_point(data=data.df, aes(x=V1, y=V2))
pic + geom_point(data=outer.df, aes(x=V1, y=V2),shape=1,colour = "red", size = 4.5)
```

### 1.4 High dimensional dataset

We will now use the code from the previous part and apply it to a harder problem in which more features describe each datapoint and only some features indicate whether a point is an outlier.

Loads the second dataset. Consisting of variables X, Xval, yval, rename them.
```{r}
ex8data2<-readMat("ex8data2.mat")
X2<-ex8data2$X
Xval2<-ex8data2$Xval
Yval2<-ex8data2$yval
```

Apply the same steps to the larger dataset:

* First, get mu and sigma of gaussian distribution
```{r}
# Estimate mu and sigma2
mu2 = apply(X2,2,mean)
sigma2_2 = apply(X2,2,sd)^2 *((dim(X2)[1]-1)/dim(X2)[1])
```
  
* Then, get p values for Training set
```{r}

dnorm.mat<-t(apply(X2,1,dnorm,mu2,sigma2_2^0.5))
p2 = apply(dnorm.mat,1,prod)

# p2 = multivariateGaussian(X2, mu2, sigma2_2)
```

* Third, get p values for Cross-validation set
```{r}
dnormval.mat<-t(apply(Xval2,1,dnorm,mu2,sigma2_2^0.5))
pval2 = apply(dnormval.mat,1,prod)

# pval2 = multivariateGaussian(Xval2, mu2, sigma2_2)
```

* Forth, Find the best threshold
```{r}
rtls2 = selectThreshold(Yval2, pval2)
```

Best epsilon found using cross-validation: `r rtls2$epsilon`


Best F1 on Cross Validation Set: `r rtls2$F1`

Outliers found:  `r sum(p2 < rtls2$epsilon)`

   (you should see a value epsilon of about 1.38e-18, F1:0.615385, outliers: 117)
   
   