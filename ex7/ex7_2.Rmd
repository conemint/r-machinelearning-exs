---
title: "Programming Exercise 7: K-means Clustering and Principal Component Analysis"
author: "Min Z"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_notebook: default
  html_document: default
---

## 2 Principal Component Analysis

In this exercise, you will use principal component analysis (PCA) to perform dimensionality reduction. You will first experiment with an example 2D dataset to get intuition on how PCA works, and then use it on a bigger dataset of 5000 face image dataset.

### 2.1 Example Dataset

First start with a 2D dataset which has one direction of large variation and one of smaller variation. In this part of the exercise, you will visualize what happens when you use PCA to reduce the data from 2D to 1D. 


```{r}
# remove all list
rm(list = ls())
```

Load an example dataset that we will be using and do simple plot
```{r}
library(R.matlab)
ex7data1 <- readMat("ex7data1.mat")
X<-ex7data1$X
plot(X)
```

### 2.2 Implementing PCA

In this part of the exercise, you will implement PCA. PCA consists of two computational steps: First, you compute the covariance matrix of the data.

Then, you compute the eigenvectors U1, U2, . . . , Un. These will correspond to the principal components of variation in the data.
```
# compute the eigenvectors
y<-eigen(A)
y$val are the eigenvalues of A
y$vec are the eigenvectors of A
```
Before running PCA, it is important to first normalize X
```{r}
featureNormalize<-function(X){
  # FEATURENORMALIZE Normalizes the features in X 
  # FEATURENORMALIZE(X) returns a normalized version of X where
  # the mean value of each feature is 0 and the standard deviation
  # is 1. This is often a good preprocessing step to do when
  # working with learning algorithms.
  
  # mean of each feature in X
  mu = apply(X,2,mean) 
  # standard deviation of each feature in X
  sigma = apply(X,2,sd) 
  
  X_norm = sweep(X,MARGIN = 2,mu,'-')
  X_norm = sweep(X_norm,MARGIN = 2,sigma,'/')

  rt_list<-list("X_norm"=X_norm, "mu"=mu, "sigma"=sigma)
  return(rt_list)
}
```

normalize X
```{r}
returnls= featureNormalize(X)
X_norm = returnls$X_norm
mu = returnls$mu
sigma = returnls$sigma
```

Now that we have X normalized, we can run PCA to compute principal components.

First, you should compute the covariance matrix of the data, which is given by:

$$\sum = \frac{1}{m}X^TX$$

where X is the data matrix with examples in rows, and m is the number of examples. Note that $\sum$ is a $n\times	n$ matrix and not the summation operator.

After computing the covariance matrix, you can run eigen() on it to compute the principal components. 

```{r}
pca_func<-function(X){
  # useful vals
  m = dim(X)[1]
  n = dim(X)[2]
  
  Sigma = (1/m)*t(X)%*%X
  y<-eigen(Sigma)
  return(list("eigenvectors"= y$vec, "eigenvalues" = y$val))
}

# %  Run PCA
pca_rs= pca_func(X_norm)
U = pca_rs$eigenvectors
S = pca_rs$eigenvalues
```

The script will output the top principal component (eigen- vector) found, and you should expect to see an output of about [-0.707 -0.707]. (It is possible that it may instead output the negative of this, since U1 and  -U1 are equally valid choices for the first principal component.)

Visualise: plot the corresponding principal components found
```{r}
library(ggplot2)

data.df<-as.data.frame(X)
p<-ggplot() + geom_point(data=data.df, aes(x=V1, y=V2))


vec_mat <- rbind(mu,mu + 1.5 * S[1] * U[,1],mu,mu + 1.5 * S[2] * U[,2])
rownames(vec_mat)<-c(1:4)
df<-as.data.frame(vec_mat)

df$grp <- as.factor(c(1,1,2,2))
p<-p+geom_line(data=df, aes(V1, V2, group = grp))+ coord_fixed(ratio = 1)
p

```

### 2.3 Dimensionality Reduction with PCA

After computing the principal components, you can use them to reduce the feature dimension of your dataset by projecting each example onto a lower dimensional space, $x^{(i)} \rightarrow z^{(i)}$. In this part of the exercise, you will use the eigenvectors returned by PCA and project the example dataset into a 1-dimensional space.

#### 2.3.1 Projecting the data onto the principal components

You are given a dataset X, the principal components U, and the desired number of dimensions to reduce to K. You should project each example in X onto the top K components in U. Note that the top K components in U are given by the first K columns of U, that is U reduce = U(:, 1:K).

```{r}
projectData<-function(X, U, K){
  # %PROJECTDATA Computes the reduced data representation when projecting only 
  # %on to the top k eigenvectors
   # Z = projectData(X, U, K) computes the projection of 
   # the normalized inputs X into the reduced dimensional space spanned by
   # the first K columns of U. It returns the projected examples in Z.
  Ur = U[, 1:K]
  Z = X%*%Ur
  return(Z)
}
```

```{r}
K=1
Z = projectData(X_norm, U, K)
```

**First example projection: `r round(Z[1],digits=3)`**


Check value: project the first example onto the first dimension and you should see a value of about 1.481 (or possibly -1.481, if you got  U1 instead of U1).

#### 2.3.2 Reconstructing an approximation of the data

After projecting the data onto the lower dimensional space, you can ap- proximately recover the data by projecting them back onto the original high dimensional space. 

```{r}
recoverData<-function(Z, U, K){
  # %RECOVERDATA Recovers an approximation of the original data when using the 
  # %projected data
  # %   X_rec = RECOVERDATA(Z, U, K) recovers an approximation the 
  # %   original data that has been reduced to K dimensions. It returns the
  # %   approximate reconstruction in X_rec.
  Ur = U[, 1:K]
  X_rec = Z%*%t(Ur)
  return(X_rec)
}
```

```{r}
X_rec  = recoverData(Z, U, K)
```

**First example recovered as: `r round(X_rec[1,],digits = 3)`**

Check value:  this will recover an approximation of the first example and you should see a value of about [-1.047 -1.047].

#### 2.3.3 Visualizing the projections

```{r}
data.df<-as.data.frame(X)
rec.df<-as.data.frame(X_rec)
q<-ggplot() + geom_point(data=data.df, aes(x=V1, y=V2))

q<-q + geom_point(data=rec.df, aes(x=V1+mu[1], y=V2+mu[2]),colour="red") + coord_fixed(ratio = 1)
q

```
#### 2.3.4 Use prcomp from stats package

```{r}
prcomp.rs<-prcomp(X_norm)
```

Now, this should give us same rotation and centers as computed above:

```{r echo=FALSE,results='asis'}
knitr::kable(prcomp.rs$rotation,caption="Rotations")
```
**First example projection: `r round(prcomp.rs$x[1],digits=3)`**

### 2.4 Face Image Dataset

In this part of the exercise, you will run PCA on face images to see how it can be used in practice for dimension reduction. The dataset ex7faces.mat contains a dataset X of face images, each 32 $\times$ 32 in grayscale. Each row of X corresponds to one face image (a row vector of length 1024).
```{r}
ex7faces <- readMat("ex7faces.mat")
Xfaces<- ex7faces$X
```

Visualize
```{r}
par(mar=c(0,0,0,0))
layout(matrix(c(1:25), 5, 5, byrow = TRUE), respect = TRUE)
sample<-sample(1:nrow(Xfaces), 25, replace=F)
for(i in sample){
  t<-matrix(Xfaces[i,],32,32,byrow=T)
  t1<-t[,32:1]
  image(t1,axes = FALSE,col=grey.colors(12))
}
```

#### 2.4.1 PCA on Faces

To run PCA on the face dataset, we first normalize the dataset by subtracting the mean of each feature from the data matrix X.

```{r}
returnls= featureNormalize(Xfaces)
Xf_norm = returnls$X_norm
```

Run PCA and visualize the eigenvectors which are in this case eigenfaces. 
We display the first 36 eigenfaces.
```{r}
# %  Run PCA
pca_rs_fc= pca_func(Xf_norm)
Uf = pca_rs_fc$eigenvectors
Sf = pca_rs_fc$eigenvalues
```

Visualize the top 36 eigenvectors found
```{r echo=FALSE}
# sample<-sample(1:nrow(Xfaces), 25, replace=F)
UF1<-Uf
UF1[,1]<--Uf[,1]
for(i in 1:10){
  t<-matrix(UF1[i,],32,32,byrow=T)
  t1<-t[,32:1]
  image(t1,col=grey.colors(12))
}
```

#### 2.4.2 Dimensionality Reduction
Project images to the eigen space using the top k eigenvectors 
If you are applying a machine learning algorithm 
```{r}
K = 100;
Z = projectData(Xf_norm, Uf, K);
```

The projected data Z has a size of: `r dim(Z)`

#### 2.4.3 Recover and visualize faces

Project images to the eigen space using the top K eigen vectors and  visualize only using those K dimensions. Compare to the original input, which is also displayed

```{r}
K = 100;
X_rec_fc  = recoverData(Z, Uf, K);
```

Original faces:
```{r}
par(mar=c(0,0,0,0))
# def.par <- par(no.readonly = TRUE) # save default, for resetting...
layout(matrix(c(1:16), 4, 4, byrow = TRUE), respect = TRUE)
for(i in 1:16){
  t<-matrix(Xf_norm[i,],32,32,byrow=T)
  t1<-t[,32:1]
  image(t1,axes = FALSE,col=grey.colors(12))
}
```

Recovered faces:
```{r out.width = "20%"}
# par(mfrow=c(5,5))
# par(mar=c(1,1,1,1))
 par(mar=c(0,0,0,0))
layout(matrix(c(1:16), 4, 4, byrow = TRUE), respect = TRUE)
# layout.show(nf)
for(i in 1:16){
  t<-matrix(X_rec_fc[i,],32,32,byrow=T)
  t1<-t[,32:1]
  image(t1,axes = FALSE,col=grey.colors(12))
  box()
}
```


### 2.5 PCA for visualization

One useful application of PCA is to use it to visualize high-dimensional data. In the last K-Means exercise you ran K-Means on 3-dimensional pixel colors of an image. We first visualize this output in 3D, and then apply PCA to obtain a visualization in 2D.

Re-load the image from the previous exercise and run K-Means on it:
```{r}
library(png)
imgbirdread = readPNG("bird_small.png")
A = imgbirdread
```

Now, re-run K-means. This time use kmeans from stats package, default algorithm:
```{r}
# K = 16
# max_iters = 10
# initial_centroidsimg = kMeansInitCentroids(X, K)
# returns = runkMeans(X, initial_centroidsimg, max_iters, FALSE)
# centroidsimg<-returns$centroids
# idximg<-returns$idx
Xbd = array(A,dim=c(dim(A)[1]*dim(A)[2],dim(A)[3]))
kmeans.rs<-kmeans(Xbd,16,iter.max = 50,nstart = 10)
idx<-kmeans.rs$cluster
centroids<-kmeans.rs$centers
```

Skip: Visualize the data and centroid memberships in 3D

Use PCA to project this cloud to 2D for visualization
 
```{r}
# % Subtract the mean to use PCA
returnls_bd= featureNormalize(Xbd)
X_norm_bd = returnls_bd$X_norm
mu_bd = returnls_bd$mu
sigma_bd = returnls_bd$sigma

# % PCA and project the data to 2D

pca_rs_bd= pca_func(X_norm_bd)
Ubd = pca_rs_bd$eigenvectors
Sbd = pca_rs_bd$eigenvalues

Z = projectData(X_norm_bd, Ubd, 2)
```

Get random sample size of 1000:
```{r}
sel<-sample(1:nrow(Xbd), 1000, replace=F)
```
Plot in 2D
```{r}
library(ggplot2)
# Set color by cond
data.df<-as.data.frame(Z[sel,])
group<-as.factor(idx[sel])
p<-ggplot() + geom_point(data=data.df, aes(x=V1, y=V2, color=group))
p
```

This visualizes the grouping results from kmeans clustering, projected to a 2D surface that the grouping can be best viewed.

The PCA projection can be thought of as a rotation that selects the view that maximizes the spread of the data, which often corresponds to the “best” view.